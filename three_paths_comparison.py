#!/usr/bin/env python3
"""
ä¸‰è·¯å¾„æ„ŸçŸ¥ç®¡é“å¯¹æ¯”æµ‹è¯•
========================

è·¯å¾„1ï¼šAARTæ··åˆæ–¹å¼ - ä½¿ç”¨AARTwareç°æœ‰ç»„ä»¶ + ä¼ ç»ŸCVè¡¥å……
è·¯å¾„2ï¼šå…¨CPU TFLite - æ‰€æœ‰æ¨ç†éƒ½ä½¿ç”¨CPU TensorFlow Lite
è·¯å¾„3ï¼šå…¨TPU - æ‰€æœ‰æ¨ç†éƒ½åœ¨EdgeTPUä¸Šæ‰§è¡Œ

æµ‹è¯•å®Œæ•´æ„ŸçŸ¥æµæ°´çº¿çš„ä¸‰ç§ä¸åŒå®ç°æ–¹å¼
"""

import os
import sys
import time
import numpy as np  
import cv2
import torch
from pathlib import Path
from typing import Dict, List, Tuple, Any
import json

# æ·»åŠ AARTwareè·¯å¾„
sys.path.append('/ros2_ws/src/workspace/AARTware-main/cores/perception/yolov8_ros2/yolo_detect_opponent/yolo_detect_opponent')

# å¯¼å…¥ä¾èµ–
try:
    from ultralytics import YOLO
    YOLO_AVAILABLE = True
except ImportError:
    YOLO_AVAILABLE = False
    print("âš ï¸  YOLOv8 ä¸å¯ç”¨")

try:
    import tflite_runtime.interpreter as tflite
    TFLITE_AVAILABLE = True
except ImportError:
    TFLITE_AVAILABLE = False
    print("âš ï¸  TFLite Runtime ä¸å¯ç”¨")

try:
    from pycoral.utils import edgetpu
    from pycoral.adapters import common, detect, segment
    TPU_AVAILABLE = True
except ImportError:
    TPU_AVAILABLE = False
    print("âš ï¸  EdgeTPU ä¸å¯ç”¨")

class SyntheticDataGenerator:
    """åˆæˆæµ‹è¯•æ•°æ®ç”Ÿæˆå™¨"""
    
    def __init__(self, width=640, height=480):
        self.width = width
        self.height = height
    
    def generate_test_scene(self) -> Tuple[np.ndarray, np.ndarray]:
        """åŠ è½½çœŸå®æµ‹è¯•å›¾ç‰‡è€Œä¸æ˜¯åˆæˆæ•°æ®"""
        # å°è¯•åŠ è½½çœŸå®å›¾ç‰‡
        real_img_path = "/ros2_ws/test_image.jpg"
        if os.path.exists(real_img_path):
            img = cv2.imread(real_img_path)
            if img is not None:
                print(f"âœ… ä½¿ç”¨çœŸå®å›¾ç‰‡: {img.shape}")
                # ç”Ÿæˆå¯¹åº”çš„æ·±åº¦å›¾ï¼ˆç®€åŒ–ç‰ˆï¼‰
                gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
                depth_map = 255 - gray  # ç®€å•çš„æ·±åº¦ä¼°è®¡
                return img, depth_map
        
        # å¦‚æœçœŸå®å›¾ç‰‡ä¸å­˜åœ¨ï¼Œåˆ›å»ºåŸºç¡€åœºæ™¯
        print("âš ï¸  çœŸå®å›¾ç‰‡ä¸å­˜åœ¨ï¼Œä½¿ç”¨åˆæˆæ•°æ®")
        img = np.zeros((self.height, self.width, 3), dtype=np.uint8)
        
        # æ·»åŠ é“è·¯èƒŒæ™¯
        cv2.rectangle(img, (0, self.height//2), (self.width, self.height), (100, 100, 100), -1)
        
        # æ·»åŠ è½¦è¾† (çº¢è‰²çŸ©å½¢)
        cv2.rectangle(img, (100, 200), (200, 300), (0, 0, 255), -1)
        cv2.rectangle(img, (400, 180), (520, 320), (0, 0, 255), -1)
        
        # æ·»åŠ è¡Œäºº (ç»¿è‰²æ¤­åœ†)
        cv2.ellipse(img, (300, 350), (30, 80), 0, 0, 360, (0, 255, 0), -1)
        
        # æ·»åŠ å™ªå£°
        noise = np.random.randint(0, 50, img.shape, dtype=np.uint8)
        img = cv2.add(img, noise)
        
        # ç”Ÿæˆå¯¹åº”çš„æ·±åº¦å›¾
        depth_map = np.full((self.height, self.width), 100, dtype=np.uint8)
        depth_map[200:300, 100:200] = 50  # è½¦è¾†1æ·±åº¦
        depth_map[180:320, 400:520] = 60  # è½¦è¾†2æ·±åº¦
        depth_map[270:430, 270:330] = 30  # è¡Œäººæ·±åº¦
        
        return img, depth_map

class Path1_AARTMixedProcessor:
    """è·¯å¾„1ï¼šAARTæ··åˆæ–¹å¼å¤„ç†å™¨"""
    
    def __init__(self):
        self.name = "è·¯å¾„1ï¼šAARTæ··åˆæ–¹å¼"
        self.description = "ä½¿ç”¨AARTwareç°æœ‰ç»„ä»¶ + ä¼ ç»ŸCVè¡¥å……"
        
        # åˆå§‹åŒ–AARTware YOLOæ£€æµ‹å™¨
        self.yolo_model = None
        if YOLO_AVAILABLE:
            try:
                # ä¼˜å…ˆä½¿ç”¨AARTwareè‡ªå®šä¹‰æ¨¡å‹
                model_paths = [
                    '/ros2_ws/yolov8n.pt',
                    '/ros2_ws/AARTware-main/cores/perception/yolov8_ros2/yolo_detect_opponent/yolo_detect_opponent/yolov8n.pt',
                    '/ros2_ws/AARTware-main/cores/perception/yolov8_ros2/yolo_detect_opponent/yolo_detect_opponent/best.pt'
                ]
                
                for model_path in model_paths:
                    if os.path.exists(model_path):
                        self.yolo_model = YOLO(model_path)
                        print(f"âœ… è·¯å¾„1ï¼šåŠ è½½YOLOv8æ¨¡å‹ {model_path}")
                        break
                else:
                    print("âš ï¸  è·¯å¾„1ï¼šYOLOv8æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨")
            except Exception as e:
                print(f"âš ï¸  è·¯å¾„1ï¼šYOLOæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        
        print(f"ğŸ”§ åˆå§‹åŒ–å®Œæˆï¼š{self.name}")
    
    def process_image(self, img: np.ndarray, depth_map: np.ndarray) -> Dict[str, Any]:
        """å¤„ç†å›¾åƒ - AARTæ··åˆæ–¹å¼"""
        results = {
            'path': self.name,
            'object_detection': [],
            'semantic_segmentation': None,
            'depth_estimation': None,
            'processing_times': {}
        }
        
        # 1. ç›®æ ‡æ£€æµ‹ - ä½¿ç”¨AARTware YOLOv8
        start_time = time.perf_counter()
        if self.yolo_model is not None:
            try:
                detections = self.yolo_model(img, verbose=False)
                for detection in detections:
                    boxes = detection.boxes
                    if boxes is not None:
                        for box in boxes:
                            conf = float(box.conf[0])
                            if conf > 0.5:
                                cls = int(box.cls[0])
                                x1, y1, x2, y2 = box.xyxy[0].tolist()
                                results['object_detection'].append({
                                    'class': f'object_{cls}',
                                    'confidence': conf,
                                    'bbox': [int(x1), int(y1), int(x2), int(y2)],
                                    'method': 'aartware_yolo'
                                })
            except Exception as e:
                print(f"è·¯å¾„1 YOLOæ£€æµ‹å¤±è´¥: {e}")
        
        # ä¼ ç»ŸCVä½œä¸ºè¡¥å……
        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
        contours, _ = cv2.findContours(gray, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        for contour in contours:
            area = cv2.contourArea(contour)
            if area > 500:  # è¿‡æ»¤å°å¯¹è±¡
                x, y, w, h = cv2.boundingRect(contour)
                results['object_detection'].append({
                    'class': 'traditional_cv_object',
                    'confidence': 0.8,
                    'bbox': [x, y, x+w, y+h],
                    'method': 'traditional_cv'
                })
        
        detection_time = (time.perf_counter() - start_time) * 1000
        results['processing_times']['object_detection'] = detection_time
        
        # 2. è¯­ä¹‰åˆ†å‰² - ä½¿ç”¨ä¼ ç»ŸCVæ–¹æ³•ï¼ˆAARTæ²¡æœ‰æ­¤åŠŸèƒ½ï¼‰
        start_time = time.perf_counter()
        segmentation_map = self._traditional_semantic_segmentation(img)
        segmentation_time = (time.perf_counter() - start_time) * 1000
        results['semantic_segmentation'] = segmentation_map
        results['processing_times']['semantic_segmentation'] = segmentation_time
        
        # 3. æ·±åº¦ä¼°è®¡ - ä½¿ç”¨ä¼ ç»ŸCVæ–¹æ³•
        start_time = time.perf_counter()
        depth_estimation = self._traditional_depth_estimation(img)
        depth_time = (time.perf_counter() - start_time) * 1000
        results['depth_estimation'] = depth_estimation
        results['processing_times']['depth_estimation'] = depth_time
        
        # è®¡ç®—æ€»æ—¶é—´
        total_time = sum(results['processing_times'].values())
        results['processing_times']['total'] = total_time
        
        return results
    
    def _traditional_semantic_segmentation(self, img: np.ndarray) -> np.ndarray:
        """ä¼ ç»ŸCVè¯­ä¹‰åˆ†å‰²"""
        hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)
        
        # åˆ›å»ºåˆ†å‰²æ©ç 
        segmentation = np.zeros(img.shape[:2], dtype=np.uint8)
        
        # é“è·¯åŒºåŸŸï¼ˆç°è‰²ï¼‰
        road_mask = cv2.inRange(hsv, (0, 0, 50), (180, 50, 150))
        segmentation[road_mask > 0] = 1
        
        # è½¦è¾†åŒºåŸŸï¼ˆçº¢è‰²ï¼‰
        car_mask = cv2.inRange(hsv, (0, 100, 100), (10, 255, 255))
        segmentation[car_mask > 0] = 2
        
        # è¡ŒäººåŒºåŸŸï¼ˆç»¿è‰²ï¼‰
        person_mask = cv2.inRange(hsv, (50, 100, 100), (70, 255, 255))
        segmentation[person_mask > 0] = 3
        
        return segmentation
    
    def _traditional_depth_estimation(self, img: np.ndarray) -> np.ndarray:
        """ä¼ ç»ŸCVæ·±åº¦ä¼°è®¡ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
        
        # ä½¿ç”¨æ¢¯åº¦å¼ºåº¦ä¼°è®¡æ·±åº¦
        grad_x = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=3)
        grad_y = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3)
        gradient_magnitude = np.sqrt(grad_x**2 + grad_y**2)
        
        # å½’ä¸€åŒ–åˆ°æ·±åº¦èŒƒå›´
        depth = 255 - cv2.normalize(gradient_magnitude, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)
        
        return depth

class Path2_CPUTFLiteProcessor:
    """è·¯å¾„2ï¼šå…¨CPU TFLiteå¤„ç†å™¨"""
    
    def __init__(self):
        self.name = "è·¯å¾„2ï¼šå…¨CPU TFLite"
        self.description = "æ‰€æœ‰æ¨ç†éƒ½ä½¿ç”¨CPU TensorFlow Lite"
        
        # æ¨¡å‹è·¯å¾„ - ä½¿ç”¨models_unifiedç»Ÿä¸€æ¨¡å‹
        self.detection_model_path = "/ros2_ws/models_unified/cpu/ssd_mobilenet_v2_300_cpu.tflite"
        self.segmentation_model_path = "/ros2_ws/models_unified/cpu/deeplabv3_mnv2_513_cpu.tflite"
        
        # åˆå§‹åŒ–æ¨¡å‹
        self.detection_interpreter = None
        self.segmentation_interpreter = None
        
        if TFLITE_AVAILABLE:
            try:
                # åŠ è½½æ£€æµ‹æ¨¡å‹
                if os.path.exists(self.detection_model_path):
                    self.detection_interpreter = tflite.Interpreter(model_path=self.detection_model_path)
                    self.detection_interpreter.allocate_tensors()
                    print("âœ… è·¯å¾„2ï¼šåŠ è½½CPUæ£€æµ‹æ¨¡å‹")
                    
                    # CPUé¢„çƒ­
                    input_details = self.detection_interpreter.get_input_details()
                    dummy_input = np.zeros(input_details[0]['shape'], dtype=input_details[0]['dtype'])
                    for _ in range(3):
                        self.detection_interpreter.set_tensor(input_details[0]['index'], dummy_input)
                        self.detection_interpreter.invoke()
                    print("âœ… è·¯å¾„2ï¼šCPUæ£€æµ‹æ¨¡å‹é¢„çƒ­å®Œæˆ")
                
                # åŠ è½½åˆ†å‰²æ¨¡å‹
                if os.path.exists(self.segmentation_model_path):
                    self.segmentation_interpreter = tflite.Interpreter(model_path=self.segmentation_model_path)
                    self.segmentation_interpreter.allocate_tensors()
                    print("âœ… è·¯å¾„2ï¼šåŠ è½½CPUåˆ†å‰²æ¨¡å‹")
                    
                    # CPUé¢„çƒ­
                    input_details = self.segmentation_interpreter.get_input_details()
                    dummy_input = np.zeros(input_details[0]['shape'], dtype=input_details[0]['dtype'])
                    for _ in range(3):
                        self.segmentation_interpreter.set_tensor(input_details[0]['index'], dummy_input)
                        self.segmentation_interpreter.invoke()
                    print("âœ… è·¯å¾„2ï¼šCPUåˆ†å‰²æ¨¡å‹é¢„çƒ­å®Œæˆ")
                    
            except Exception as e:
                print(f"âš ï¸  è·¯å¾„2ï¼šæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        
        print(f"ğŸ”§ åˆå§‹åŒ–å®Œæˆï¼š{self.name}")
    
    def process_image(self, img: np.ndarray, depth_map: np.ndarray) -> Dict[str, Any]:
        """å¤„ç†å›¾åƒ - å…¨CPU TFLite"""
        results = {
            'path': self.name,
            'object_detection': [],
            'semantic_segmentation': None,
            'depth_estimation': None,
            'processing_times': {},
            'detailed_times': {}
        }
        
        # 1. ç›®æ ‡æ£€æµ‹ - CPU TFLite
        if self.detection_interpreter is not None:
            detection_result = self._cpu_object_detection_detailed(img)
            results['object_detection'] = detection_result['detections']
            results['processing_times']['object_detection'] = detection_result['total_time']
            results['detailed_times']['object_detection'] = detection_result['detailed_times']
        else:
            results['processing_times']['object_detection'] = 0
            results['detailed_times']['object_detection'] = {}
        
        # 2. è¯­ä¹‰åˆ†å‰² - CPU TFLite
        if self.segmentation_interpreter is not None:
            segmentation_result = self._cpu_semantic_segmentation_detailed(img)
            results['semantic_segmentation'] = segmentation_result['segmentation']
            results['processing_times']['semantic_segmentation'] = segmentation_result['total_time']
            results['detailed_times']['semantic_segmentation'] = segmentation_result['detailed_times']
        else:
            results['processing_times']['semantic_segmentation'] = 0
            results['detailed_times']['semantic_segmentation'] = {}
        
        # 3. æ·±åº¦ä¼°è®¡ - ä½¿ç”¨ç›¸åŒçš„ä¼ ç»ŸCVæ–¹æ³•æ§åˆ¶å˜é‡
        start_time = time.perf_counter()
        depth_estimation = self._cpu_depth_estimation(img)
        depth_time = (time.perf_counter() - start_time) * 1000
        results['depth_estimation'] = depth_estimation
        results['processing_times']['depth_estimation'] = depth_time
        
        # æ·±åº¦ä¼°è®¡ä½¿ç”¨ç›¸åŒçš„ä¼ ç»ŸCVæ–¹æ³•ï¼Œå®é™…æµ‹é‡æ—¶é—´ï¼Œä¸éœ€è¦è¯¦ç»†åˆ†è§£
        
        # è®¡ç®—æ€»æ—¶é—´
        total_time = sum(results['processing_times'].values())
        results['processing_times']['total'] = total_time
        
        return results
    
    def _cpu_object_detection_detailed(self, img: np.ndarray) -> Dict[str, Any]:
        """CPU TFLiteç›®æ ‡æ£€æµ‹ - è¯¦ç»†æ—¶é—´ç»Ÿè®¡"""
        try:
            detailed_times = {}
            total_start = time.perf_counter()
            
            # è·å–è¾“å…¥è¾“å‡ºè¯¦æƒ…
            input_details = self.detection_interpreter.get_input_details()
            output_details = self.detection_interpreter.get_output_details()
            
            # 1. é¢„å¤„ç†
            preprocessing_start = time.perf_counter()
            input_shape = input_details[0]['shape']
            height, width = input_shape[1], input_shape[2]
            img_resized = cv2.resize(img, (width, height))
            img_rgb = cv2.cvtColor(img_resized, cv2.COLOR_BGR2RGB)
            
            if input_details[0]['dtype'] == np.uint8:
                input_data = np.expand_dims(img_rgb.astype(np.uint8), axis=0)
            else:
                img_normalized = img_rgb.astype(np.float32) / 255.0
                input_data = np.expand_dims(img_normalized, axis=0)
            
            detailed_times['preprocessing'] = (time.perf_counter() - preprocessing_start) * 1000
            
            # 2. æ•°æ®ä¼ è¾“
            data_transfer_start = time.perf_counter()
            self.detection_interpreter.set_tensor(input_details[0]['index'], input_data)
            detailed_times['data_transfer'] = (time.perf_counter() - data_transfer_start) * 1000
            
            # 3. çº¯æ¨ç†
            inference_start = time.perf_counter()
            self.detection_interpreter.invoke()
            detailed_times['inference'] = (time.perf_counter() - inference_start) * 1000
            
            # 4. ç»“æœè·å–
            result_fetch_start = time.perf_counter()
            boxes = self.detection_interpreter.get_tensor(output_details[0]['index'])[0]
            classes = self.detection_interpreter.get_tensor(output_details[1]['index'])[0]
            scores = self.detection_interpreter.get_tensor(output_details[2]['index'])[0]
            detailed_times['result_fetch'] = (time.perf_counter() - result_fetch_start) * 1000
            
            # 5. åå¤„ç†
            postprocessing_start = time.perf_counter()
            detections = []
            for i in range(len(scores)):
                if scores[i] > 0.3:  # é™ä½ç½®ä¿¡åº¦é˜ˆå€¼
                    y1, x1, y2, x2 = boxes[i]
                    x1, y1, x2, y2 = int(x1 * img.shape[1]), int(y1 * img.shape[0]), int(x2 * img.shape[1]), int(y2 * img.shape[0])
                    detections.append({
                        'class': int(classes[i]),
                        'score': float(scores[i]),
                        'bbox': [x1, y1, x2, y2]
                    })
            detailed_times['postprocessing'] = (time.perf_counter() - postprocessing_start) * 1000
            
            # æ€»æ—¶é—´
            total_time = (time.perf_counter() - total_start) * 1000
            
            return {
                'detections': detections,
                'total_time': total_time,
                'detailed_times': detailed_times
            }
            
        except Exception as e:
            print(f"CPUæ£€æµ‹å¤±è´¥: {e}")
            return {
                'detections': [],
                'total_time': 0,
                'detailed_times': {}
            }
    
    def _cpu_object_detection(self, img: np.ndarray) -> List[Dict]:
        """CPU TFLiteç›®æ ‡æ£€æµ‹"""
        try:
            # è·å–è¾“å…¥è¾“å‡ºè¯¦æƒ…
            input_details = self.detection_interpreter.get_input_details()
            output_details = self.detection_interpreter.get_output_details()
            
            # é¢„å¤„ç†å›¾åƒ
            input_shape = input_details[0]['shape']
            height, width = input_shape[1], input_shape[2]
            
            # è°ƒæ•´å›¾åƒå¤§å°å¹¶å¤„ç†é‡åŒ–æ¨¡å‹
            img_resized = cv2.resize(img, (width, height))
            img_rgb = cv2.cvtColor(img_resized, cv2.COLOR_BGR2RGB)
            
            # æ£€æŸ¥æ¨¡å‹æœŸæœ›çš„æ•°æ®ç±»å‹
            if input_details[0]['dtype'] == np.uint8:
                # é‡åŒ–æ¨¡å‹ï¼Œä½¿ç”¨uint8è¾“å…¥
                input_data = np.expand_dims(img_rgb.astype(np.uint8), axis=0)
            else:
                # æµ®ç‚¹æ¨¡å‹ï¼Œä½¿ç”¨å½’ä¸€åŒ–è¾“å…¥
                img_normalized = img_rgb.astype(np.float32) / 255.0
                input_data = np.expand_dims(img_normalized, axis=0)
            
            # è¿è¡Œæ¨ç†
            self.detection_interpreter.set_tensor(input_details[0]['index'], input_data)
            self.detection_interpreter.invoke()
            
            # è·å–è¾“å‡º
            boxes = self.detection_interpreter.get_tensor(output_details[0]['index'])[0]
            classes = self.detection_interpreter.get_tensor(output_details[1]['index'])[0]
            scores = self.detection_interpreter.get_tensor(output_details[2]['index'])[0]
            
            # å¤„ç†æ£€æµ‹ç»“æœ
            detections = []
            for i in range(len(scores)):
                if scores[i] > 0.3:  # é™ä½ç½®ä¿¡åº¦é˜ˆå€¼
                    y1, x1, y2, x2 = boxes[i]
                    x1, y1, x2, y2 = int(x1 * img.shape[1]), int(y1 * img.shape[0]), int(x2 * img.shape[1]), int(y2 * img.shape[0])
                    
                    detections.append({
                        'class': f'class_{int(classes[i])}',
                        'confidence': float(scores[i]),
                        'bbox': [x1, y1, x2, y2],
                        'method': 'cpu_tflite'
                    })
            
            return detections
        
        except Exception as e:
            print(f"è·¯å¾„2 CPUæ£€æµ‹å¤±è´¥: {e}")
            return []
    
    def _cpu_semantic_segmentation_detailed(self, img: np.ndarray) -> Dict[str, Any]:
        """CPU TFLiteè¯­ä¹‰åˆ†å‰² - è¯¦ç»†æ—¶é—´ç»Ÿè®¡"""
        try:
            detailed_times = {}
            total_start = time.perf_counter()
            
            # è·å–è¾“å…¥è¾“å‡ºè¯¦æƒ…
            input_details = self.segmentation_interpreter.get_input_details()
            output_details = self.segmentation_interpreter.get_output_details()
            
            # 1. é¢„å¤„ç†
            preprocessing_start = time.perf_counter()
            input_shape = input_details[0]['shape']
            height, width = input_shape[1], input_shape[2]
            img_resized = cv2.resize(img, (width, height))
            img_rgb = cv2.cvtColor(img_resized, cv2.COLOR_BGR2RGB)
            
            if input_details[0]['dtype'] == np.uint8:
                input_data = np.expand_dims(img_rgb.astype(np.uint8), axis=0)
            else:
                img_normalized = img_rgb.astype(np.float32) / 127.5 - 1.0
                input_data = np.expand_dims(img_normalized, axis=0)
            
            detailed_times['preprocessing'] = (time.perf_counter() - preprocessing_start) * 1000
            
            # 2. æ•°æ®ä¼ è¾“
            data_transfer_start = time.perf_counter()
            self.segmentation_interpreter.set_tensor(input_details[0]['index'], input_data)
            detailed_times['data_transfer'] = (time.perf_counter() - data_transfer_start) * 1000
            
            # 3. çº¯æ¨ç†
            inference_start = time.perf_counter()
            self.segmentation_interpreter.invoke()
            detailed_times['inference'] = (time.perf_counter() - inference_start) * 1000
            
            # 4. ç»“æœè·å–
            result_fetch_start = time.perf_counter()
            output_data = self.segmentation_interpreter.get_tensor(output_details[0]['index'])[0]
            detailed_times['result_fetch'] = (time.perf_counter() - result_fetch_start) * 1000
            
            # 5. åå¤„ç†
            postprocessing_start = time.perf_counter()
            # è¾“å‡ºå·²ç»æ˜¯åˆ†å‰²æ©ç ï¼Œæ— éœ€argmax
            segmentation_map = output_data  # output_dataå·²ç»æ˜¯[513,513]æ ¼å¼
            segmentation_resized = cv2.resize(segmentation_map.astype(np.uint8), (img.shape[1], img.shape[0]), interpolation=cv2.INTER_NEAREST)
            detailed_times['postprocessing'] = (time.perf_counter() - postprocessing_start) * 1000
            
            # æ€»æ—¶é—´
            total_time = (time.perf_counter() - total_start) * 1000
            
            return {
                'segmentation': segmentation_resized,
                'total_time': total_time,
                'detailed_times': detailed_times
            }
            
        except Exception as e:
            print(f"è·¯å¾„2 CPUåˆ†å‰²å¤±è´¥: {e}")
            return {
                'segmentation': np.zeros((img.shape[0], img.shape[1]), dtype=np.uint8),
                'total_time': 0,
                'detailed_times': {}
            }
    
    def _cpu_semantic_segmentation(self, img: np.ndarray) -> np.ndarray:
        """CPU TFLiteè¯­ä¹‰åˆ†å‰²"""
        try:
            # è·å–è¾“å…¥è¾“å‡ºè¯¦æƒ…
            input_details = self.segmentation_interpreter.get_input_details()
            output_details = self.segmentation_interpreter.get_output_details()
            
            # é¢„å¤„ç†å›¾åƒ
            input_shape = input_details[0]['shape']
            height, width = input_shape[1], input_shape[2]
            
            # è°ƒæ•´å›¾åƒå¤§å°å¹¶å¤„ç†é‡åŒ–æ¨¡å‹
            img_resized = cv2.resize(img, (width, height))
            img_rgb = cv2.cvtColor(img_resized, cv2.COLOR_BGR2RGB)
            
            # æ£€æŸ¥æ¨¡å‹æœŸæœ›çš„æ•°æ®ç±»å‹
            if input_details[0]['dtype'] == np.uint8:
                # é‡åŒ–æ¨¡å‹ï¼Œä½¿ç”¨uint8è¾“å…¥
                input_data = np.expand_dims(img_rgb.astype(np.uint8), axis=0)
            else:
                # æµ®ç‚¹æ¨¡å‹ï¼Œä½¿ç”¨å½’ä¸€åŒ–è¾“å…¥
                img_normalized = img_rgb.astype(np.float32) / 127.5 - 1.0
                input_data = np.expand_dims(img_normalized, axis=0)
            
            # è¿è¡Œæ¨ç†
            self.segmentation_interpreter.set_tensor(input_details[0]['index'], input_data)
            self.segmentation_interpreter.invoke()
            
            # è·å–è¾“å‡º
            output_data = self.segmentation_interpreter.get_tensor(output_details[0]['index'])[0]
            
            # è¾“å‡ºå·²ç»æ˜¯åˆ†å‰²æ©ç ï¼Œæ— éœ€argmax
            segmentation_map = output_data  # output_dataå·²ç»æ˜¯[513,513]æ ¼å¼
            
            # è°ƒæ•´å›åŸå§‹å°ºå¯¸
            segmentation_resized = cv2.resize(segmentation_map.astype(np.uint8), (img.shape[1], img.shape[0]), interpolation=cv2.INTER_NEAREST)
            
            return segmentation_resized
            
        except Exception as e:
            print(f"è·¯å¾„2 CPUåˆ†å‰²å¤±è´¥: {e}")
            return np.zeros((img.shape[0], img.shape[1]), dtype=np.uint8)
    
    def _cpu_depth_estimation(self, img: np.ndarray) -> np.ndarray:
        """CPUæ·±åº¦ä¼°è®¡ï¼ˆç»Ÿä¸€ä½¿ç”¨Sobelæ–¹æ³•ï¼‰"""
        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
        
        # ä½¿ç”¨æ¢¯åº¦å¼ºåº¦ä¼°è®¡æ·±åº¦ï¼ˆä¸Path 1ç›¸åŒï¼‰
        grad_x = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=3)
        grad_y = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3)
        gradient_magnitude = np.sqrt(grad_x**2 + grad_y**2)
        
        # å½’ä¸€åŒ–åˆ°æ·±åº¦èŒƒå›´
        depth = 255 - cv2.normalize(gradient_magnitude, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)
        
        return depth

class Path3_TPUProcessor:
    """è·¯å¾„3ï¼šå…¨TPUå¤„ç†å™¨"""
    
    def __init__(self):
        self.name = "è·¯å¾„3ï¼šå…¨TPU"
        self.description = "æ‰€æœ‰æ¨ç†éƒ½åœ¨EdgeTPUä¸Šæ‰§è¡Œ"
        
        # æ¨¡å‹è·¯å¾„ - ä½¿ç”¨models_unifiedç»Ÿä¸€æ¨¡å‹
        self.detection_model_path = "/ros2_ws/models_unified/edgetpu/ssd_mobilenet_v2_300_edgetpu.tflite"
        self.segmentation_model_path = "/ros2_ws/models_unified/edgetpu/deeplabv3_mnv2_513_edgetpu.tflite"
        
        # åˆå§‹åŒ–æ¨¡å‹
        self.detection_interpreter = None
        self.segmentation_interpreter = None
        
        if TPU_AVAILABLE:
            try:
                # åŠ è½½æ£€æµ‹æ¨¡å‹
                if os.path.exists(self.detection_model_path):
                    self.detection_interpreter = tflite.Interpreter(
                        model_path=self.detection_model_path,
                        experimental_delegates=[edgetpu.load_edgetpu_delegate()]
                    )
                    self.detection_interpreter.allocate_tensors()
                    print("âœ… è·¯å¾„3ï¼šåŠ è½½TPUæ£€æµ‹æ¨¡å‹")
                    
                    # EdgeTPUé¢„çƒ­
                    input_details = self.detection_interpreter.get_input_details()
                    dummy_input = np.zeros(input_details[0]['shape'], dtype=input_details[0]['dtype'])
                    for _ in range(5):
                        self.detection_interpreter.set_tensor(input_details[0]['index'], dummy_input)
                        self.detection_interpreter.invoke()
                    print("âœ… è·¯å¾„3ï¼šTPUæ£€æµ‹æ¨¡å‹é¢„çƒ­å®Œæˆ")
                
                # åŠ è½½åˆ†å‰²æ¨¡å‹
                if os.path.exists(self.segmentation_model_path):
                    self.segmentation_interpreter = tflite.Interpreter(
                        model_path=self.segmentation_model_path,
                        experimental_delegates=[edgetpu.load_edgetpu_delegate()]
                    )
                    self.segmentation_interpreter.allocate_tensors()
                    print("âœ… è·¯å¾„3ï¼šåŠ è½½TPUåˆ†å‰²æ¨¡å‹")
                    
                    # EdgeTPUé¢„çƒ­
                    input_details = self.segmentation_interpreter.get_input_details()
                    dummy_input = np.zeros(input_details[0]['shape'], dtype=input_details[0]['dtype'])
                    for _ in range(5):
                        self.segmentation_interpreter.set_tensor(input_details[0]['index'], dummy_input)
                        self.segmentation_interpreter.invoke()
                    print("âœ… è·¯å¾„3ï¼šTPUåˆ†å‰²æ¨¡å‹é¢„çƒ­å®Œæˆ")
                    
            except Exception as e:
                print(f"âš ï¸  è·¯å¾„3ï¼šTPUæ¨¡å‹åŠ è½½å¤±è´¥ï¼Œå›é€€åˆ°CPU: {e}")
                # å›é€€åˆ°CPU
                try:
                    if os.path.exists(self.detection_model_path):
                        self.detection_interpreter = tflite.Interpreter(model_path=self.detection_model_path)
                        self.detection_interpreter.allocate_tensors()
                        print("âœ… è·¯å¾„3ï¼šä½¿ç”¨CPUæ¨¡å¼")
                except Exception as e2:
                    print(f"âš ï¸  è·¯å¾„3ï¼šCPUå›é€€ä¹Ÿå¤±è´¥: {e2}")
        
        print(f"ğŸ”§ åˆå§‹åŒ–å®Œæˆï¼š{self.name}")
    
    def process_image(self, img: np.ndarray, depth_map: np.ndarray) -> Dict[str, Any]:
        """å¤„ç†å›¾åƒ - å…¨TPU"""
        results = {
            'path': self.name,
            'object_detection': [],
            'semantic_segmentation': None,
            'depth_estimation': None,
            'processing_times': {},
            'detailed_times': {}
        }
        
        # 1. ç›®æ ‡æ£€æµ‹ - TPU
        if self.detection_interpreter is not None:
            detection_result = self._tpu_object_detection_detailed(img)
            results['object_detection'] = detection_result['detections']
            results['processing_times']['object_detection'] = detection_result['total_time']
            results['detailed_times']['object_detection'] = detection_result['detailed_times']
        else:
            results['processing_times']['object_detection'] = 0
            results['detailed_times']['object_detection'] = {}
        
        # 2. è¯­ä¹‰åˆ†å‰² - TPU
        if self.segmentation_interpreter is not None:
            segmentation_result = self._tpu_semantic_segmentation_detailed(img)
            results['semantic_segmentation'] = segmentation_result['segmentation']
            results['processing_times']['semantic_segmentation'] = segmentation_result['total_time']
            results['detailed_times']['semantic_segmentation'] = segmentation_result['detailed_times']
        else:
            results['processing_times']['semantic_segmentation'] = 0
            results['detailed_times']['semantic_segmentation'] = {}
        
        # 3. æ·±åº¦ä¼°è®¡ - ä½¿ç”¨ç›¸åŒçš„ä¼ ç»ŸCVæ–¹æ³•æ§åˆ¶å˜é‡
        start_time = time.perf_counter()
        depth_estimation = self._tpu_depth_estimation(img)
        depth_time = (time.perf_counter() - start_time) * 1000
        results['depth_estimation'] = depth_estimation
        results['processing_times']['depth_estimation'] = depth_time
        
        # æ·±åº¦ä¼°è®¡ä½¿ç”¨ç›¸åŒçš„ä¼ ç»ŸCVæ–¹æ³•ï¼Œå®é™…æµ‹é‡æ—¶é—´ï¼Œä¸éœ€è¦è¯¦ç»†åˆ†è§£
        
        # è®¡ç®—æ€»æ—¶é—´
        total_time = sum(results['processing_times'].values())
        results['processing_times']['total'] = total_time
        
        return results
    
    def _tpu_object_detection_detailed(self, img: np.ndarray) -> Dict[str, Any]:
        """TPUç›®æ ‡æ£€æµ‹ - è¯¦ç»†æ—¶é—´ç»Ÿè®¡"""
        try:
            detailed_times = {}
            total_start = time.perf_counter()
            
            # è·å–è¾“å…¥è¾“å‡ºè¯¦æƒ…
            input_details = self.detection_interpreter.get_input_details()
            output_details = self.detection_interpreter.get_output_details()
            
            # 1. é¢„å¤„ç†
            preprocessing_start = time.perf_counter()
            input_shape = input_details[0]['shape']
            height, width = input_shape[1], input_shape[2]
            img_resized = cv2.resize(img, (width, height))
            img_rgb = cv2.cvtColor(img_resized, cv2.COLOR_BGR2RGB)
            
            if input_details[0]['dtype'] == np.uint8:
                input_data = np.expand_dims(img_rgb.astype(np.uint8), axis=0)
            else:
                img_normalized = img_rgb.astype(np.float32) / 255.0
                input_data = np.expand_dims(img_normalized, axis=0)
            
            detailed_times['preprocessing'] = (time.perf_counter() - preprocessing_start) * 1000
            
            # 2. æ•°æ®ä¼ è¾“
            data_transfer_start = time.perf_counter()
            self.detection_interpreter.set_tensor(input_details[0]['index'], input_data)
            detailed_times['data_transfer'] = (time.perf_counter() - data_transfer_start) * 1000
            
            # 3. çº¯æ¨ç†
            inference_start = time.perf_counter()
            self.detection_interpreter.invoke()
            detailed_times['inference'] = (time.perf_counter() - inference_start) * 1000
            
            # 4. ç»“æœè·å–
            result_fetch_start = time.perf_counter()
            boxes = self.detection_interpreter.get_tensor(output_details[0]['index'])[0]
            classes = self.detection_interpreter.get_tensor(output_details[1]['index'])[0]
            scores = self.detection_interpreter.get_tensor(output_details[2]['index'])[0]
            detailed_times['result_fetch'] = (time.perf_counter() - result_fetch_start) * 1000
            
            # 5. åå¤„ç†
            postprocessing_start = time.perf_counter()
            detections = []
            for i in range(len(scores)):
                if scores[i] > 0.3:  # é™ä½ç½®ä¿¡åº¦é˜ˆå€¼
                    y1, x1, y2, x2 = boxes[i]
                    x1, y1, x2, y2 = int(x1 * img.shape[1]), int(y1 * img.shape[0]), int(x2 * img.shape[1]), int(y2 * img.shape[0])
                    detections.append({
                        'class': f'class_{int(classes[i])}',
                        'confidence': float(scores[i]),
                        'bbox': [x1, y1, x2, y2],
                        'method': 'edgetpu'
                    })
            detailed_times['postprocessing'] = (time.perf_counter() - postprocessing_start) * 1000
            
            # æ€»æ—¶é—´
            total_time = (time.perf_counter() - total_start) * 1000
            
            # 6. ç«‹å³å†è¿è¡Œä¸€éæµ‹é‡ç†æƒ³æ€§èƒ½ï¼ˆæ— é¢„çƒ­å¼€é”€ï¼‰
            ideal_start = time.perf_counter()
            self.detection_interpreter.set_tensor(input_details[0]['index'], input_data)
            self.detection_interpreter.invoke()
            ideal_time = (time.perf_counter() - ideal_start) * 1000
            detailed_times['ideal_total'] = ideal_time
            
            # å•ç‹¬æµ‹é‡ç†æƒ³æ¨ç†æ—¶é—´
            ideal_inference_start = time.perf_counter()
            self.detection_interpreter.invoke()
            ideal_inference_time = (time.perf_counter() - ideal_inference_start) * 1000
            detailed_times['ideal_inference'] = ideal_inference_time
            
            return {
                'detections': detections,
                'total_time': total_time,
                'detailed_times': detailed_times
            }
            
        except Exception as e:
            print(f"è·¯å¾„3 TPUæ£€æµ‹å¤±è´¥: {e}")
            return {
                'detections': [],
                'total_time': 0,
                'detailed_times': {}
            }
    
    def _tpu_object_detection(self, img: np.ndarray) -> List[Dict]:
        """TPUç›®æ ‡æ£€æµ‹"""
        try:
            # è·å–è¾“å…¥è¾“å‡ºè¯¦æƒ…
            input_details = self.detection_interpreter.get_input_details()
            output_details = self.detection_interpreter.get_output_details()
            
            # é¢„å¤„ç†å›¾åƒ
            input_shape = input_details[0]['shape']
            height, width = input_shape[1], input_shape[2]
            
            # è°ƒæ•´å›¾åƒå¤§å°å¹¶å¤„ç†é‡åŒ–æ¨¡å‹
            img_resized = cv2.resize(img, (width, height))
            img_rgb = cv2.cvtColor(img_resized, cv2.COLOR_BGR2RGB)
            
            # æ£€æŸ¥æ¨¡å‹æœŸæœ›çš„æ•°æ®ç±»å‹
            if input_details[0]['dtype'] == np.uint8:
                # é‡åŒ–æ¨¡å‹ï¼Œä½¿ç”¨uint8è¾“å…¥
                input_data = np.expand_dims(img_rgb.astype(np.uint8), axis=0)
            else:
                # æµ®ç‚¹æ¨¡å‹ï¼Œä½¿ç”¨å½’ä¸€åŒ–è¾“å…¥
                img_normalized = img_rgb.astype(np.float32) / 255.0
                input_data = np.expand_dims(img_normalized, axis=0)
            
            # è¿è¡Œæ¨ç†
            self.detection_interpreter.set_tensor(input_details[0]['index'], input_data)
            self.detection_interpreter.invoke()
            
            # è·å–è¾“å‡º
            boxes = self.detection_interpreter.get_tensor(output_details[0]['index'])[0]
            classes = self.detection_interpreter.get_tensor(output_details[1]['index'])[0]
            scores = self.detection_interpreter.get_tensor(output_details[2]['index'])[0]
            
            # å¤„ç†æ£€æµ‹ç»“æœ
            detections = []
            for i in range(len(scores)):
                if scores[i] > 0.3:  # é™ä½ç½®ä¿¡åº¦é˜ˆå€¼
                    y1, x1, y2, x2 = boxes[i]
                    x1, y1, x2, y2 = int(x1 * img.shape[1]), int(y1 * img.shape[0]), int(x2 * img.shape[1]), int(y2 * img.shape[0])
                    
                    detections.append({
                        'class': f'class_{int(classes[i])}',
                        'confidence': float(scores[i]),
                        'bbox': [x1, y1, x2, y2],
                        'method': 'tpu_edgetpu'
                    })
            
            return detections
        
        except Exception as e:
            print(f"è·¯å¾„3 TPUæ£€æµ‹å¤±è´¥: {e}")
            return []
    
    def _tpu_semantic_segmentation_detailed(self, img: np.ndarray) -> Dict[str, Any]:
        """TPUè¯­ä¹‰åˆ†å‰² - è¯¦ç»†æ—¶é—´ç»Ÿè®¡"""
        try:
            detailed_times = {}
            total_start = time.perf_counter()
            
            # è·å–è¾“å…¥è¾“å‡ºè¯¦æƒ…
            input_details = self.segmentation_interpreter.get_input_details()
            output_details = self.segmentation_interpreter.get_output_details()
            
            # 1. é¢„å¤„ç†
            preprocessing_start = time.perf_counter()
            input_shape = input_details[0]['shape']
            height, width = input_shape[1], input_shape[2]
            img_resized = cv2.resize(img, (width, height))
            img_rgb = cv2.cvtColor(img_resized, cv2.COLOR_BGR2RGB)
            
            if input_details[0]['dtype'] == np.uint8:
                input_data = np.expand_dims(img_rgb.astype(np.uint8), axis=0)
            else:
                img_normalized = img_rgb.astype(np.float32) / 127.5 - 1.0
                input_data = np.expand_dims(img_normalized, axis=0)
            
            detailed_times['preprocessing'] = (time.perf_counter() - preprocessing_start) * 1000
            
            # 2. æ•°æ®ä¼ è¾“
            data_transfer_start = time.perf_counter()
            self.segmentation_interpreter.set_tensor(input_details[0]['index'], input_data)
            detailed_times['data_transfer'] = (time.perf_counter() - data_transfer_start) * 1000
            
            # 3. çº¯æ¨ç†
            inference_start = time.perf_counter()
            self.segmentation_interpreter.invoke()
            detailed_times['inference'] = (time.perf_counter() - inference_start) * 1000
            
            # 4. ç»“æœè·å–
            result_fetch_start = time.perf_counter()
            output_data = self.segmentation_interpreter.get_tensor(output_details[0]['index'])[0]
            detailed_times['result_fetch'] = (time.perf_counter() - result_fetch_start) * 1000
            
            # 5. åå¤„ç†
            postprocessing_start = time.perf_counter()
            # è¾“å‡ºå·²ç»æ˜¯åˆ†å‰²æ©ç ï¼Œæ— éœ€argmax
            segmentation_map = output_data  # output_dataå·²ç»æ˜¯[513,513]æ ¼å¼
            segmentation_resized = cv2.resize(segmentation_map.astype(np.uint8), (img.shape[1], img.shape[0]), interpolation=cv2.INTER_NEAREST)
            detailed_times['postprocessing'] = (time.perf_counter() - postprocessing_start) * 1000
            
            # æ€»æ—¶é—´
            total_time = (time.perf_counter() - total_start) * 1000
            
            # 6. ç«‹å³å†è¿è¡Œä¸€éæµ‹é‡ç†æƒ³æ€§èƒ½ï¼ˆæ— é¢„çƒ­å¼€é”€ï¼‰
            ideal_start = time.perf_counter()
            self.segmentation_interpreter.set_tensor(input_details[0]['index'], input_data)
            self.segmentation_interpreter.invoke()
            ideal_time = (time.perf_counter() - ideal_start) * 1000
            detailed_times['ideal_total'] = ideal_time
            
            # å•ç‹¬æµ‹é‡ç†æƒ³æ¨ç†æ—¶é—´
            ideal_inference_start = time.perf_counter()
            self.segmentation_interpreter.invoke()
            ideal_inference_time = (time.perf_counter() - ideal_inference_start) * 1000
            detailed_times['ideal_inference'] = ideal_inference_time
            
            return {
                'segmentation': segmentation_resized,
                'total_time': total_time,
                'detailed_times': detailed_times
            }
            
        except Exception as e:
            print(f"è·¯å¾„3 TPUåˆ†å‰²å¤±è´¥: {e}")
            return {
                'segmentation': np.zeros((img.shape[0], img.shape[1]), dtype=np.uint8),
                'total_time': 0,
                'detailed_times': {}
            }
    
    def _tpu_semantic_segmentation(self, img: np.ndarray) -> np.ndarray:
        """TPUè¯­ä¹‰åˆ†å‰²"""
        try:
            # è·å–è¾“å…¥è¾“å‡ºè¯¦æƒ…
            input_details = self.segmentation_interpreter.get_input_details()
            output_details = self.segmentation_interpreter.get_output_details()
            
            # é¢„å¤„ç†å›¾åƒ
            input_shape = input_details[0]['shape']
            height, width = input_shape[1], input_shape[2]
            
            # è°ƒæ•´å›¾åƒå¤§å°å¹¶å¤„ç†é‡åŒ–æ¨¡å‹
            img_resized = cv2.resize(img, (width, height))
            img_rgb = cv2.cvtColor(img_resized, cv2.COLOR_BGR2RGB)
            
            # æ£€æŸ¥æ¨¡å‹æœŸæœ›çš„æ•°æ®ç±»å‹
            if input_details[0]['dtype'] == np.uint8:
                # é‡åŒ–æ¨¡å‹ï¼Œä½¿ç”¨uint8è¾“å…¥
                input_data = np.expand_dims(img_rgb.astype(np.uint8), axis=0)
            else:
                # æµ®ç‚¹æ¨¡å‹ï¼Œä½¿ç”¨å½’ä¸€åŒ–è¾“å…¥
                img_normalized = img_rgb.astype(np.float32) / 127.5 - 1.0
                input_data = np.expand_dims(img_normalized, axis=0)
            
            # è¿è¡Œæ¨ç†
            self.segmentation_interpreter.set_tensor(input_details[0]['index'], input_data)
            self.segmentation_interpreter.invoke()
            
            # è·å–è¾“å‡º
            output_data = self.segmentation_interpreter.get_tensor(output_details[0]['index'])[0]
            
            # è¾“å‡ºå·²ç»æ˜¯åˆ†å‰²æ©ç ï¼Œæ— éœ€argmax
            segmentation_map = output_data  # output_dataå·²ç»æ˜¯[513,513]æ ¼å¼
            
            # è°ƒæ•´å›åŸå§‹å°ºå¯¸
            segmentation_resized = cv2.resize(segmentation_map.astype(np.uint8), (img.shape[1], img.shape[0]), interpolation=cv2.INTER_NEAREST)
            
            return segmentation_resized
            
        except Exception as e:
            print(f"è·¯å¾„3 TPUåˆ†å‰²å¤±è´¥: {e}")
            return np.zeros((img.shape[0], img.shape[1]), dtype=np.uint8)
    
    def _tpu_depth_estimation(self, img: np.ndarray) -> np.ndarray:
        """TPUæ·±åº¦ä¼°è®¡ï¼ˆç»Ÿä¸€ä½¿ç”¨Sobelæ–¹æ³•ï¼‰"""
        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
        
        # ä½¿ç”¨æ¢¯åº¦å¼ºåº¦ä¼°è®¡æ·±åº¦ï¼ˆä¸Path 1ã€2ç›¸åŒï¼‰
        grad_x = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=3)
        grad_y = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3)
        gradient_magnitude = np.sqrt(grad_x**2 + grad_y**2)
        
        # å½’ä¸€åŒ–åˆ°æ·±åº¦èŒƒå›´
        depth = 255 - cv2.normalize(gradient_magnitude, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)
        
        return depth

class ThreePathsComparison:
    """ä¸‰è·¯å¾„å¯¹æ¯”æµ‹è¯•å™¨"""
    
    def __init__(self):
        self.data_generator = SyntheticDataGenerator()
        
        # åˆå§‹åŒ–ä¸‰ä¸ªå¤„ç†å™¨
        self.path1_processor = Path1_AARTMixedProcessor()
        self.path2_processor = Path2_CPUTFLiteProcessor()
        self.path3_processor = Path3_TPUProcessor()
        
        self.processors = [
            self.path1_processor,
            self.path2_processor,
            self.path3_processor
        ]
    
    def run_comparison(self, num_tests: int = 3, save_results: bool = True) -> Dict[str, Any]:
        """è¿è¡Œä¸‰è·¯å¾„å¯¹æ¯”æµ‹è¯•"""
        print("\n" + "="*60)
        print("ğŸš€ ä¸‰è·¯å¾„æ„ŸçŸ¥ç®¡é“å¯¹æ¯”æµ‹è¯•")
        print("="*60)
        
        all_results = {}
        
        for processor in self.processors:
            print(f"\nğŸ”§ æµ‹è¯• {processor.name}")
            print(f"   æè¿°: {processor.description}")
            
            path_results = {
                'processor': processor,
                'results': [],
                'avg_times': {},
                'total_detections': 0
            }
            
            # è¿è¡Œå¤šæ¬¡æµ‹è¯•
            for i in range(num_tests):
                print(f"   æµ‹è¯•è½®æ¬¡ {i+1}/{num_tests}")
                
                # ç”Ÿæˆæµ‹è¯•æ•°æ®
                test_img, test_depth = self.data_generator.generate_test_scene()
                
                # è¿è¡Œå¤„ç†
                try:
                    result = processor.process_image(test_img, test_depth)
                    path_results['results'].append(result)
                    path_results['total_detections'] += len(result['object_detection'])
                    
                    # æ‰“å°æœ¬è½®ç»“æœ
                    total_time = result['processing_times']['total']
                    detection_count = len(result['object_detection'])
                    print(f"     å¤„ç†æ—¶é—´: {total_time:.2f}ms, æ£€æµ‹æ•°é‡: {detection_count}")
                    
                except Exception as e:
                    print(f"     âŒ å¤„ç†å¤±è´¥: {e}")
                    continue
            
            # è®¡ç®—å¹³å‡æ—¶é—´
            if path_results['results']:
                avg_times = {}
                for key in path_results['results'][0]['processing_times'].keys():
                    times = [r['processing_times'][key] for r in path_results['results']]
                    avg_times[key] = sum(times) / len(times)
                path_results['avg_times'] = avg_times
            
            all_results[processor.name] = path_results
        
        # æ‰“å°å¯¹æ¯”ç»“æœ
        self._print_comparison_results(all_results)
        
        # ä¿å­˜ç»“æœ
        if save_results:
            self._save_results(all_results)
        
        return all_results
    
    def _print_comparison_results(self, results: Dict[str, Any]):
        """æ‰“å°å¯¹æ¯”ç»“æœ"""
        print("\n" + "="*60)
        print("ğŸ“Š ä¸‰è·¯å¾„æ€§èƒ½å¯¹æ¯”ç»“æœ")
        print("="*60)
        
        print(f"{'è·¯å¾„':<20} {'æ€»æ—¶é—´(ms)':<12} {'æ£€æµ‹æ—¶é—´(ms)':<14} {'åˆ†å‰²æ—¶é—´(ms)':<14} {'æ·±åº¦æ—¶é—´(ms)':<14} {'æ£€æµ‹æ•°é‡':<8}")
        print("-" * 95)
        
        for path_name, path_data in results.items():
            if path_data['avg_times']:
                avg_times = path_data['avg_times']
                total_time = avg_times.get('total', 0)
                detection_time = avg_times.get('object_detection', 0)
                segmentation_time = avg_times.get('semantic_segmentation', 0)
                depth_time = avg_times.get('depth_estimation', 0)
                total_detections = path_data['total_detections']
                
                print(f"{path_name:<20} {total_time:<12.2f} {detection_time:<14.2f} {segmentation_time:<14.2f} {depth_time:<14.2f} {total_detections:<8}")
        
        # æ·»åŠ è¯¦ç»†æ—¶é—´åˆ†è§£
        self._print_detailed_timing_breakdown(results)
        
        print("\nğŸ¯ æ€§èƒ½åˆ†æ:")
        
        # æ‰¾å‡ºæœ€å¿«çš„è·¯å¾„
        fastest_path = min(results.items(), 
                          key=lambda x: x[1]['avg_times'].get('total', float('inf')) if x[1]['avg_times'] else float('inf'))
        
        if fastest_path[1]['avg_times']:
            print(f"   æœ€å¿«è·¯å¾„: {fastest_path[0]} ({fastest_path[1]['avg_times']['total']:.2f}ms)")
        
        # åŠŸèƒ½å¯¹æ¯”
        print("\nğŸ” åŠŸèƒ½å¯¹æ¯”:")
        print("   è·¯å¾„1 (AARTæ··åˆ)ï¼šâœ… ç›®æ ‡æ£€æµ‹(AARTware) âœ… è¯­ä¹‰åˆ†å‰²(ä¼ ç»ŸCV) âœ… æ·±åº¦ä¼°è®¡(ä¼ ç»ŸCV)")
        print("   è·¯å¾„2 (CPU TFLite)ï¼šâœ… ç›®æ ‡æ£€æµ‹(TFLite) âœ… è¯­ä¹‰åˆ†å‰²(TFLite) âœ… æ·±åº¦ä¼°è®¡(ä¼ ç»ŸCV)")
        print("   è·¯å¾„3 (TPU)ï¼šâœ… ç›®æ ‡æ£€æµ‹(EdgeTPU) âœ… è¯­ä¹‰åˆ†å‰²(EdgeTPU) âœ… æ·±åº¦ä¼°è®¡(å¿«é€Ÿç®—æ³•)")
        
        print("\nğŸ“ˆ æ¨èä½¿ç”¨åœºæ™¯:")
        print("   è·¯å¾„1ï¼šå¼€å‘åŸå‹ï¼Œä½¿ç”¨ç°æœ‰AARTwareä»£ç ")
        print("   è·¯å¾„2ï¼šç”Ÿäº§éƒ¨ç½²ï¼ŒCPUè®¾å¤‡ï¼ŒåŠŸèƒ½å®Œæ•´")
        print("   è·¯å¾„3ï¼šé«˜æ€§èƒ½éœ€æ±‚ï¼Œæœ‰EdgeTPUç¡¬ä»¶")
    
    def _print_detailed_timing_breakdown(self, results: Dict[str, Any]):
        """æ‰“å°è¯¦ç»†æ—¶é—´åˆ†è§£ - æŒ‰ç”¨æˆ·è¦æ±‚çš„æ ¼å¼"""
        print("\n" + "="*80)
        print("ğŸ” åˆ†æ­¥åŠŸèƒ½æ¨¡å—è¯¦ç»†æ—¶é—´åˆ†è§£åˆ†æ")
        print("="*80)
        
        # è·å–å„è·¯å¾„æ•°æ®
        path1_data = results.get('è·¯å¾„1ï¼šAARTæ··åˆæ–¹å¼', {})
        path2_data = results.get('è·¯å¾„2ï¼šå…¨CPU TFLite', {})  
        path3_data = results.get('è·¯å¾„3ï¼šå…¨TPU', {})
        
        if not (path1_data or path2_data or path3_data):
            print("âš ï¸  æ²¡æœ‰è¯¦ç»†æ—¶é—´æ•°æ®å¯ä¾›åˆ†æ")
            return
            
        # æ­¥éª¤1ï¼šç›®æ ‡æ£€æµ‹
        print("\nğŸ“Š æ­¥éª¤1ï¼šç›®æ ‡æ£€æµ‹ (SSD MobileNet V2)")
        print("="*60)
        self._print_step_comparison('object_detection', path1_data, path2_data, path3_data)
        
        # æ­¥éª¤2ï¼šè¯­ä¹‰åˆ†å‰²
        print("\nğŸ“Š æ­¥éª¤2ï¼šè¯­ä¹‰åˆ†å‰² (DeepLab V3)")
        print("="*60)
        self._print_step_comparison('semantic_segmentation', path1_data, path2_data, path3_data)
        
        # æ­¥éª¤3ï¼šæ·±åº¦ä¼°è®¡
        print("\nğŸ“Š æ­¥éª¤3ï¼šæ·±åº¦ä¼°è®¡ (ä¼ ç»ŸCV)")
        print("="*60)
        self._print_step_comparison('depth_estimation', path1_data, path2_data, path3_data)
        
        # æœ€åï¼šæ€»æ—¶é—´æ±‡æ€»
        self._print_final_summary(results)
    
    def _print_step_comparison(self, step_key: str, path1_data: Dict, path2_data: Dict, path3_data: Dict):
        """æŒ‰æ­¥éª¤æ‰“å°ä¸‰è·¯å¾„å¯¹æ¯”"""
        
        # è·¯å¾„1ï¼šAARTæ··åˆæ–¹å¼
        path1_time = path1_data.get('avg_times', {}).get(step_key, 0)
        print(f"è·¯å¾„1 (AART YOLO+CV): {path1_time:.2f}ms")
        
        # è·¯å¾„2ï¼šCPU TFLite
        path2_detailed = path2_data.get('detailed_times', {}).get(step_key, {})
        path2_total = sum(v for k, v in path2_detailed.items() if not k.startswith('ideal'))
        if path2_detailed:
            print(f"è·¯å¾„2 (CPU TFLite):")
            print(f"  â”œâ”€â”€ é¢„å¤„ç†: {path2_detailed.get('preprocessing', 0):.2f}ms")
            print(f"  â”œâ”€â”€ æ•°æ®ä¼ è¾“: {path2_detailed.get('data_transfer', 0):.2f}ms")
            print(f"  â”œâ”€â”€ çº¯æ¨ç†: {path2_detailed.get('inference', 0):.2f}ms")
            print(f"  â”œâ”€â”€ ç»“æœè·å–: {path2_detailed.get('result_fetch', 0):.2f}ms")
            print(f"  â”œâ”€â”€ åå¤„ç†: {path2_detailed.get('postprocessing', 0):.2f}ms")
            print(f"  â””â”€â”€ æ€»æ—¶é—´: {path2_total:.2f}ms")
        else:
            path2_time = path2_data.get('avg_times', {}).get(step_key, 0)
            print(f"è·¯å¾„2 (CPU TFLite): {path2_time:.2f}ms")
        
        # è·¯å¾„3ï¼šEdgeTPU
        path3_detailed = path3_data.get('detailed_times', {}).get(step_key, {})
        path3_total = sum(v for k, v in path3_detailed.items() if not k.startswith('ideal'))
        if path3_detailed:
            print(f"è·¯å¾„3 (EdgeTPU):")
            print(f"  â”œâ”€â”€ é¢„å¤„ç†: {path3_detailed.get('preprocessing', 0):.2f}ms")
            print(f"  â”œâ”€â”€ æ•°æ®ä¼ è¾“: {path3_detailed.get('data_transfer', 0):.2f}ms")
            print(f"  â”œâ”€â”€ çº¯æ¨ç†: {path3_detailed.get('inference', 0):.2f}ms")
            print(f"  â”œâ”€â”€ ç»“æœè·å–: {path3_detailed.get('result_fetch', 0):.2f}ms")
            print(f"  â”œâ”€â”€ åå¤„ç†: {path3_detailed.get('postprocessing', 0):.2f}ms")
            print(f"  â”œâ”€â”€ æ€»æ—¶é—´: {path3_total:.2f}ms")
            
            # ç†æƒ³æ€§èƒ½ï¼ˆçƒ­åŠ è½½ï¼‰
            ideal_total = path3_detailed.get('ideal_total', 0)
            ideal_inference = path3_detailed.get('ideal_inference', 0)
            if ideal_total > 0:
                print(f"  ğŸš€ ç†æƒ³æ€»æ—¶é—´: {ideal_total:.2f}ms")
            if ideal_inference > 0:
                print(f"  ğŸš€ ç†æƒ³æ¨ç†: {ideal_inference:.2f}ms")
        else:
            path3_time = path3_data.get('avg_times', {}).get(step_key, 0)
            print(f"è·¯å¾„3 (EdgeTPU): {path3_time:.2f}ms")
        
        # æ€§èƒ½å¯¹æ¯”
        if path2_total > 0 and path3_total > 0:
            speedup = path2_total / path3_total
            print(f"\nğŸš€ CPU vs EdgeTPU: {speedup:.2f}x åŠ é€Ÿ")
            
        # ç†æƒ³æ€§èƒ½å¯¹æ¯”
        if path3_detailed and 'ideal_total' in path3_detailed and path2_total > 0:
            ideal_speedup = path2_total / path3_detailed['ideal_total']
            print(f"ğŸš€ ç†æƒ³åŠ é€Ÿæ¯”: {ideal_speedup:.2f}x")
    
    def _print_final_summary(self, results: Dict[str, Any]):
        """æœ€ç»ˆæ€»æ—¶é—´æ±‡æ€»"""
        print("\n" + "="*80)
        print("ğŸ“Š æœ€ç»ˆæ€»æ—¶é—´æ±‡æ€»")
        print("="*80)
        
        for path_name, path_data in results.items():
            if not path_data.get('avg_times'):
                continue
                
            avg_times = path_data['avg_times']
            detection_time = avg_times.get('object_detection', 0)
            segmentation_time = avg_times.get('semantic_segmentation', 0)
            depth_time = avg_times.get('depth_estimation', 0)
            total_time = detection_time + segmentation_time + depth_time
            
            print(f"\n{path_name}:")
            print(f"  â”œâ”€â”€ ç›®æ ‡æ£€æµ‹: {detection_time:.2f}ms")
            print(f"  â”œâ”€â”€ è¯­ä¹‰åˆ†å‰²: {segmentation_time:.2f}ms")
            print(f"  â”œâ”€â”€ æ·±åº¦ä¼°è®¡: {depth_time:.2f}ms")
            print(f"  â””â”€â”€ æ€»æ—¶é—´: {total_time:.2f}ms")
            
            # è·¯å¾„3çš„ç†æƒ³æ€§èƒ½
            if path_name == 'è·¯å¾„3ï¼šå…¨TPU':
                detailed_times = path_data.get('detailed_times', {})
                ideal_detection = detailed_times.get('object_detection', {}).get('ideal_total', 0)
                ideal_segmentation = detailed_times.get('semantic_segmentation', {}).get('ideal_total', 0)
                ideal_total = ideal_detection + ideal_segmentation + depth_time
                
                if ideal_detection > 0 or ideal_segmentation > 0:
                    print(f"  ğŸš€ ç†æƒ³çŠ¶æ€:")
                    print(f"     â”œâ”€â”€ ç›®æ ‡æ£€æµ‹: {ideal_detection:.2f}ms")
                    print(f"     â”œâ”€â”€ è¯­ä¹‰åˆ†å‰²: {ideal_segmentation:.2f}ms")
                    print(f"     â”œâ”€â”€ æ·±åº¦ä¼°è®¡: {depth_time:.2f}ms")
                    print(f"     â””â”€â”€ ç†æƒ³æ€»æ—¶é—´: {ideal_total:.2f}ms")
    
    def _print_module_comparison(self, title: str, module_key: str, path1_data: Dict, path2_data: Dict, path3_data: Dict):
        """æ‰“å°å•ä¸ªæ¨¡å—çš„å¯¹æ¯”"""
        print(f"\n{'='*80}")
        print(f"ğŸ“Š {title}")
        print('='*80)
        
        # è·¯å¾„1æ—¶é—´ï¼ˆæ€»æ—¶é—´ï¼Œæ— è¯¦ç»†åˆ†è§£ï¼‰
        path1_time = path1_data.get('avg_times', {}).get(module_key, 0)
        if path1_time > 0:
            print(f"è·¯å¾„1 (AARTwareæ··åˆ): {path1_time:.2f}ms")
        else:
            print(f"è·¯å¾„1 (AARTwareæ··åˆ): æ— æ•°æ®")
        
        # è·¯å¾„2è¯¦ç»†æ—¶é—´
        path2_detailed = path2_data.get('detailed_times', {}).get(module_key, {})
        if path2_detailed:
            print(f"\nè·¯å¾„2 (CPU TFLite) è¯¦ç»†åˆ†è§£:")
            print(f"  â”œâ”€â”€ é¢„å¤„ç†: {path2_detailed.get('preprocessing', 0):.2f}ms")
            print(f"  â”œâ”€â”€ æ•°æ®ä¼ è¾“: {path2_detailed.get('data_transfer', 0):.2f}ms")
            print(f"  â”œâ”€â”€ çº¯æ¨ç†: {path2_detailed.get('inference', 0):.2f}ms")
            print(f"  â”œâ”€â”€ ç»“æœè·å–: {path2_detailed.get('result_fetch', 0):.2f}ms")
            print(f"  â”œâ”€â”€ åå¤„ç†: {path2_detailed.get('postprocessing', 0):.2f}ms")
            path2_total = sum(v for k, v in path2_detailed.items() if not k.startswith('ideal'))
            print(f"  â””â”€â”€ æ€»è®¡: {path2_total:.2f}ms")
        else:
            path2_time = path2_data.get('avg_times', {}).get(module_key, 0)
            if path2_time > 0:
                print(f"\nè·¯å¾„2 (CPU TFLite): {path2_time:.2f}ms")
                path2_total = path2_time
            else:
                print(f"\nè·¯å¾„2 (CPU TFLite): æ— æ•°æ®")
                path2_total = 0
        
        # è·¯å¾„3è¯¦ç»†æ—¶é—´
        path3_detailed = path3_data.get('detailed_times', {}).get(module_key, {})
        if path3_detailed:
            print(f"\nè·¯å¾„3 (EdgeTPU) è¯¦ç»†åˆ†è§£:")
            print(f"  â”œâ”€â”€ é¢„å¤„ç†: {path3_detailed.get('preprocessing', 0):.2f}ms")
            print(f"  â”œâ”€â”€ æ•°æ®ä¼ è¾“: {path3_detailed.get('data_transfer', 0):.2f}ms")
            print(f"  â”œâ”€â”€ çº¯æ¨ç†: {path3_detailed.get('inference', 0):.2f}ms")
            print(f"  â”œâ”€â”€ ç»“æœè·å–: {path3_detailed.get('result_fetch', 0):.2f}ms")
            print(f"  â”œâ”€â”€ åå¤„ç†: {path3_detailed.get('postprocessing', 0):.2f}ms")
            path3_total = sum(v for k, v in path3_detailed.items() if not k.startswith('ideal'))
            print(f"  â””â”€â”€ æ€»è®¡: {path3_total:.2f}ms")
            
            # ç†æƒ³æ€§èƒ½
            if 'ideal_total' in path3_detailed or 'ideal_inference' in path3_detailed:
                print(f"\nè·¯å¾„3 (EdgeTPU) ç†æƒ³æ€§èƒ½:")
                if 'ideal_total' in path3_detailed:
                    print(f"  ğŸš€ ç†æƒ³æ€»æ—¶é—´: {path3_detailed.get('ideal_total', 0):.2f}ms")
                if 'ideal_inference' in path3_detailed:
                    print(f"  ğŸš€ ç†æƒ³æ¨ç†: {path3_detailed.get('ideal_inference', 0):.2f}ms")
        else:
            path3_time = path3_data.get('avg_times', {}).get(module_key, 0)
            if path3_time > 0:
                print(f"\nè·¯å¾„3 (EdgeTPU): {path3_time:.2f}ms")
                path3_total = path3_time
            else:
                print(f"\nè·¯å¾„3 (EdgeTPU): æ— æ•°æ®")
                path3_total = 0
        
        # æ€§èƒ½å¯¹æ¯”åˆ†æ
        print(f"\nğŸ¯ {title.split('ï¼š')[1]} æ€§èƒ½å¯¹æ¯”:")
        print("-" * 70)
        
        if path1_time > 0 and path2_total > 0:
            print(f"è·¯å¾„1 vs è·¯å¾„2: {path1_time:.2f}ms vs {path2_total:.2f}ms = {path1_time/path2_total:.2f}x")
        if path1_time > 0 and path3_total > 0:
            print(f"è·¯å¾„1 vs è·¯å¾„3: {path1_time:.2f}ms vs {path3_total:.2f}ms = {path1_time/path3_total:.2f}x")
        
        if path2_total > 0 and path3_total > 0:
            cpu_vs_tpu = path2_total / path3_total
            print(f"è·¯å¾„2 vs è·¯å¾„3: {path2_total:.2f}ms vs {path3_total:.2f}ms = {cpu_vs_tpu:.2f}x åŠ é€Ÿ")
        
        # ç†æƒ³æ€§èƒ½å¯¹æ¯”
        if path3_detailed and 'ideal_inference' in path3_detailed and path2_detailed and 'inference' in path2_detailed:
            ideal_speedup = path2_detailed.get('inference', 0) / path3_detailed.get('ideal_inference', 1)
            print(f"ğŸš€ CPU vs EdgeTPUç†æƒ³: {path2_detailed.get('inference', 0):.2f}ms vs {path3_detailed.get('ideal_inference', 0):.2f}ms = {ideal_speedup:.2f}x ç†æƒ³åŠ é€Ÿ")
        
        # å¦‚æœæ‰€æœ‰æ•°æ®éƒ½ä¸º0ï¼Œæ˜¾ç¤ºæç¤º
        if path1_time == 0 and path2_total == 0 and path3_total == 0:
            print("âš ï¸  è¯¥æ¨¡å—æš‚æ— æ€§èƒ½æ•°æ®")
    
    def _print_total_pipeline_comparison(self, results: Dict[str, Any]):
        """æ‰“å°æ€»æµç¨‹å¯¹æ¯”"""
        print(f"\n{'='*80}")
        print("ğŸ“Š æ€»æµç¨‹å¯¹æ¯”åˆ†æ")
        print('='*80)
        
        print("æµç¨‹æè¿°:")
        print("  è·¯å¾„1: YOLOv8æ£€æµ‹ â†’ ä¼ ç»ŸCVåˆ†å‰² â†’ ä¼ ç»ŸCVæ·±åº¦ä¼°è®¡")
        print("  è·¯å¾„2: CPU TFLiteæ£€æµ‹ â†’ CPU TFLiteåˆ†å‰² â†’ ä¼ ç»ŸCVæ·±åº¦ä¼°è®¡") 
        print("  è·¯å¾„3: EdgeTPUæ£€æµ‹ â†’ EdgeTPUåˆ†å‰² â†’ å¿«é€Ÿæ·±åº¦ä¼°è®¡")
        
        print(f"\nå„è·¯å¾„æ€»æ—¶é—´å¯¹æ¯”:")
        print("-" * 70)
        
        for path_name, path_data in results.items():
            if path_data.get('avg_time') is not None:
                avg_time = path_data['avg_time']
                std_time = path_data.get('std_time', 0)
                detections = path_data.get('total_detections', 0)
                
                print(f"{path_name}:")
                print(f"  â”œâ”€â”€ å¹³å‡æ—¶é—´: {avg_time:.2f}Â±{std_time:.2f}ms")
                print(f"  â”œâ”€â”€ æ£€æµ‹æ€»æ•°: {detections}")
                
                # æ˜¾ç¤ºå„æ­¥éª¤æ—¶é—´
                if 'avg_times' in path_data:
                    step_times = path_data['avg_times']
                    print(f"  â”œâ”€â”€ ç›®æ ‡æ£€æµ‹: {step_times.get('object_detection', 0):.2f}ms")
                    print(f"  â”œâ”€â”€ è¯­ä¹‰åˆ†å‰²: {step_times.get('semantic_segmentation', 0):.2f}ms")
                    print(f"  â””â”€â”€ æ·±åº¦ä¼°è®¡: {step_times.get('depth_estimation', 0):.2f}ms")
                
                # ç†æƒ³æ€§èƒ½æ±‡æ€»
                if path_name == 'è·¯å¾„3ï¼šå…¨TPU' and path_data.get('detailed_times'):
                    ideal_detection = path_data['detailed_times'].get('object_detection', {}).get('ideal_total', 0)
                    ideal_segmentation = path_data['detailed_times'].get('semantic_segmentation', {}).get('ideal_total', 0)
                    ideal_depth = step_times.get('depth_estimation', 0)  # æ·±åº¦ä¼°è®¡æ²¡æœ‰ç†æƒ³ç‰ˆæœ¬
                    ideal_total = ideal_detection + ideal_segmentation + ideal_depth
                    
                    if ideal_total > 0:
                        print(f"  ğŸš€ ç†æƒ³æ€»æ—¶é—´: {ideal_total:.2f}ms (æ£€æµ‹{ideal_detection:.2f} + åˆ†å‰²{ideal_segmentation:.2f} + æ·±åº¦{ideal_depth:.2f})")
        
        # æœ€ç»ˆæ€§èƒ½å¯¹æ¯”  
        path_times = {}
        ideal_time = 0
        
        for path_name, path_data in results.items():
            if path_data.get('avg_time') is not None:
                path_times[path_name] = path_data['avg_time']
                
                # è®¡ç®—ç†æƒ³æ—¶é—´
                if path_name == 'è·¯å¾„3ï¼šå…¨TPU':
                    detailed = path_data.get('detailed_times', {})
                    ideal_detection = detailed.get('object_detection', {}).get('ideal_total', 0)
                    ideal_segmentation = detailed.get('semantic_segmentation', {}).get('ideal_total', 0)
                    ideal_depth = path_data.get('avg_times', {}).get('depth_estimation', 0)
                    ideal_time = ideal_detection + ideal_segmentation + ideal_depth
        
        print(f"\nğŸ¯ æ€»ä½“æ€§èƒ½æ’å:")
        print("-" * 70)
        sorted_paths = sorted([(name, time) for name, time in path_times.items()], key=lambda x: x[1])
        
        for i, (path_name, time) in enumerate(sorted_paths, 1):
            print(f"{i}. {path_name}: {time:.2f}ms")
        
        if ideal_time > 0:
            print(f"ğŸš€ ç†æƒ³EdgeTPU: {ideal_time:.2f}ms")
        
        # åŠ é€Ÿæ¯”å¯¹æ¯”
        if len(path_times) >= 2:
            print(f"\nğŸš€ åŠ é€Ÿæ¯”åˆ†æ:")
            print("-" * 70)
            slowest_time = max(path_times.values())
            for path_name, time in path_times.items():
                speedup = slowest_time / time
                print(f"{path_name} ç›¸å¯¹æœ€æ…¢è·¯å¾„: {speedup:.2f}x")
            
            if ideal_time > 0:
                ideal_speedup = slowest_time / ideal_time
                print(f"ğŸš€ ç†æƒ³EdgeTPU ç›¸å¯¹æœ€æ…¢è·¯å¾„: {ideal_speedup:.2f}x")
    
    def _save_results(self, results: Dict[str, Any]):
        """ä¿å­˜ç»“æœåˆ°æ–‡ä»¶"""
        timestamp = int(time.time())
        filename = f"/ros2_ws/three_paths_comparison_{timestamp}.json"
        
        # å‡†å¤‡å¯åºåˆ—åŒ–çš„æ•°æ®
        serializable_results = {}
        for path_name, path_data in results.items():
            serializable_results[path_name] = {
                'description': path_data['processor'].description,
                'avg_times': path_data['avg_times'],
                'total_detections': path_data['total_detections'],
                'num_tests': len(path_data['results'])
            }
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(serializable_results, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {filename}")

def test_individual_path(path_name: str, num_tests: int = 3):
    """å•ç‹¬æµ‹è¯•ä¸€ä¸ªè·¯å¾„ï¼Œé¿å…å†…å­˜é—®é¢˜"""
    print(f"\nğŸš€ å¼€å§‹æµ‹è¯• {path_name}")
    print("="*40)
    
    try:
        # åªåˆå§‹åŒ–æŒ‡å®šè·¯å¾„çš„å¤„ç†å™¨
        if path_name == "è·¯å¾„1":
            processor = Path1_AARTMixedProcessor()
        elif path_name == "è·¯å¾„2": 
            processor = Path2_CPUTFLiteProcessor()
        elif path_name == "è·¯å¾„3":
            processor = Path3_TPUProcessor()
        else:
            print(f"âŒ æœªçŸ¥è·¯å¾„: {path_name}")
            return None
        
        print(f"âœ… {path_name} ({processor.description}) åˆå§‹åŒ–å®Œæˆ")
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        data_generator = SyntheticDataGenerator()
        
        # è¿è¡Œæµ‹è¯•
        results = []
        times = []
        
        for i in range(num_tests):
            print(f"   è¿è¡Œç¬¬ {i+1}/{num_tests} æ¬¡æµ‹è¯•...")
            
            # ç”Ÿæˆæµ‹è¯•æ•°æ®
            test_img, test_depth = data_generator.generate_test_scene()
            
            start_time = time.perf_counter()
            result = processor.process_image(test_img, test_depth)
            end_time = time.perf_counter()
            
            processing_time = (end_time - start_time) * 1000
            times.append(processing_time)
            results.append(result)
            
            detection_count = len(result['object_detection']) if 'object_detection' in result else 0
            print(f"   ç¬¬{i+1}æ¬¡: {processing_time:.2f}ms, æ£€æµ‹åˆ° {detection_count} ä¸ªç‰©ä½“")
        
        avg_time = np.mean(times)
        std_time = np.std(times)
        total_detections = sum(len(r.get('object_detection', [])) for r in results)
        
        # è®¡ç®—è¯¦ç»†æ—¶é—´çš„å¹³å‡å€¼
        detailed_times = {}
        if results and 'detailed_times' in results[0]:
            for task in ['object_detection', 'semantic_segmentation', 'depth_estimation']:
                if task in results[0]['detailed_times']:
                    detailed_times[task] = {}
                    for timing_type in ['preprocessing', 'data_transfer', 'inference', 'result_fetch', 'postprocessing', 'ideal_total', 'ideal_inference']:
                        timing_values = [r['detailed_times'][task].get(timing_type, 0) for r in results if r.get('detailed_times', {}).get(task, {}).get(timing_type, 0) > 0]
                        if timing_values:
                            detailed_times[task][timing_type] = np.mean(timing_values)
        
        # å¯¹äºè·¯å¾„1ï¼Œè®¡ç®—processing_timesçš„å¹³å‡å€¼
        processing_times = {}
        if results and 'processing_times' in results[0]:
            for task in ['object_detection', 'semantic_segmentation', 'depth_estimation']:
                if task in results[0]['processing_times']:
                    timing_values = [r['processing_times'][task] for r in results if r.get('processing_times', {}).get(task, 0) > 0]
                    if timing_values:
                        processing_times[task] = np.mean(timing_values)
        
        summary = {
            'path_name': path_name,
            'description': processor.description,
            'times': times,
            'avg_time': avg_time,
            'std_time': std_time,
            'total_detections': total_detections,
            'num_tests': num_tests,
            'detailed_times': detailed_times,
            'processing_times': processing_times
        }
        
        print(f"âœ… {path_name} å®Œæˆ - å¹³å‡è€—æ—¶: {avg_time:.2f}Â±{std_time:.2f}ms")
        print(f"   æ€»æ£€æµ‹æ•°: {total_detections}")
        
        # æ¸…ç†å†…å­˜
        del processor
        del data_generator
        import gc
        gc.collect()
        
        return summary
        
    except Exception as e:
        print(f"âŒ {path_name} æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None

def main():
    """ä¸»å‡½æ•° - åˆ†æ­¥æµ‹è¯•æ¨¡å¼"""
    print("ğŸ¯ ä¸‰è·¯å¾„æ„ŸçŸ¥ç®¡é“å¯¹æ¯”æµ‹è¯• (å†…å­˜å‹å¥½æ¨¡å¼)")
    print("="*50)
    
    # æ£€æŸ¥ä¾èµ–
    print("ğŸ“‹ æ£€æŸ¥ä¾èµ–:")
    print(f"   YOLOv8: {'âœ…' if YOLO_AVAILABLE else 'âŒ'}")
    print(f"   TFLite: {'âœ…' if TFLITE_AVAILABLE else 'âŒ'}")
    print(f"   EdgeTPU: {'âœ…' if TPU_AVAILABLE else 'âŒ'}")
    
    # åˆ†æ­¥æµ‹è¯•æ¯ä¸ªè·¯å¾„
    test_paths = ["è·¯å¾„2", "è·¯å¾„3", "è·¯å¾„1"]  # å…ˆæµ‹è¯•è½»é‡çº§çš„
    results = {}
    
    for path_name in test_paths:
        result = test_individual_path(path_name, num_tests=3)
        if result:
            results[path_name] = result
        
        # æ¯æ¬¡æµ‹è¯•åç­‰å¾…å†…å­˜å›æ”¶
        print("ğŸ• ç­‰å¾…å†…å­˜å›æ”¶...")
        time.sleep(2)
    
    # æŒ‰ç”¨æˆ·è¦æ±‚çš„æ ¼å¼è¾“å‡º
    if len(results) > 1:
        print("\n" + "="*50)
        print("æµ‹è¯•ç»“æœ")
        print("="*50)
        
        # æ­¥éª¤1ï¼šæ£€æµ‹
        print("\næ­¥éª¤1ï¼šæ£€æµ‹")
        print("-" * 30)
        
        if "è·¯å¾„1" in results:
            # è·¯å¾„1ä½¿ç”¨processing_timesç»“æ„
            if 'processing_times' in results["è·¯å¾„1"]:
                detection_time = results["è·¯å¾„1"]['processing_times'].get('object_detection', 0)
                print(f"è·¯å¾„1(AART yolo+cv): {detection_time:.2f}ms")
            else:
                print("è·¯å¾„1(AART yolo+cv): æ•°æ®ä¸å¯ç”¨")
        
        if "è·¯å¾„2" in results:
            path2_detection = results["è·¯å¾„2"].get('detailed_times', {}).get('object_detection', {})
            if path2_detection:
                preprocessing = path2_detection.get('preprocessing', 0)
                data_transfer = path2_detection.get('data_transfer', 0)
                inference = path2_detection.get('inference', 0)
                result_fetch = path2_detection.get('result_fetch', 0)
                postprocessing = path2_detection.get('postprocessing', 0)
                total = preprocessing + data_transfer + inference + result_fetch + postprocessing
                print(f"è·¯å¾„2(cpu+tflite): {total:.2f}ms")
            else:
                print("è·¯å¾„2(cpu+tflite): æ•°æ®ä¸å¯ç”¨")
        
        if "è·¯å¾„3" in results:
            path3_detection = results["è·¯å¾„3"].get('detailed_times', {}).get('object_detection', {})
            if path3_detection:
                preprocessing = path3_detection.get('preprocessing', 0)
                data_transfer = path3_detection.get('data_transfer', 0)
                inference = path3_detection.get('inference', 0)
                result_fetch = path3_detection.get('result_fetch', 0)
                postprocessing = path3_detection.get('postprocessing', 0)
                total = preprocessing + data_transfer + inference + result_fetch + postprocessing
                
                ideal_inference = path3_detection.get('ideal_inference', 0)
                ideal_total = preprocessing + data_transfer + ideal_inference + result_fetch + postprocessing
                
                print(f"è·¯å¾„3(tpu+tflite): é¢„å¤„ç†{preprocessing:.2f}ms + æ•°æ®ä¼ è¾“{data_transfer:.2f}ms + æ¨ç†{inference:.2f}ms(ä¸éœ€è¦åŠ è½½{ideal_inference:.2f}ms) + ç»“æœè·å–{result_fetch:.2f}ms + åå¤„ç†{postprocessing:.2f}ms = {total:.2f}ms/ä¸éœ€è¦åŠ è½½{ideal_total:.2f}ms")
            else:
                print("è·¯å¾„3(tpu+tflite): æ•°æ®ä¸å¯ç”¨")
        
        # æ­¥éª¤2ï¼šè¯­ä¹‰åˆ†å‰²
        print("\næ­¥éª¤2ï¼šè¯­ä¹‰åˆ†å‰²")
        print("-" * 30)
        
        if "è·¯å¾„1" in results:
            # è·¯å¾„1ä½¿ç”¨processing_timesç»“æ„
            if 'processing_times' in results["è·¯å¾„1"]:
                segmentation_time = results["è·¯å¾„1"]['processing_times'].get('semantic_segmentation', 0)
                print(f"è·¯å¾„1(AART yolo+cv): {segmentation_time:.2f}ms")
            else:
                print("è·¯å¾„1(AART yolo+cv): æ•°æ®ä¸å¯ç”¨")
        
        if "è·¯å¾„2" in results:
            path2_segmentation = results["è·¯å¾„2"].get('detailed_times', {}).get('semantic_segmentation', {})
            if path2_segmentation:
                preprocessing = path2_segmentation.get('preprocessing', 0)
                data_transfer = path2_segmentation.get('data_transfer', 0)
                inference = path2_segmentation.get('inference', 0)
                result_fetch = path2_segmentation.get('result_fetch', 0)
                postprocessing = path2_segmentation.get('postprocessing', 0)
                total = preprocessing + data_transfer + inference + result_fetch + postprocessing
                print(f"è·¯å¾„2(cpu+tflite): {total:.2f}ms")
            else:
                print("è·¯å¾„2(cpu+tflite): æ•°æ®ä¸å¯ç”¨")
        
        if "è·¯å¾„3" in results:
            path3_segmentation = results["è·¯å¾„3"].get('detailed_times', {}).get('semantic_segmentation', {})
            if path3_segmentation:
                preprocessing = path3_segmentation.get('preprocessing', 0)
                data_transfer = path3_segmentation.get('data_transfer', 0)
                inference = path3_segmentation.get('inference', 0)
                result_fetch = path3_segmentation.get('result_fetch', 0)
                postprocessing = path3_segmentation.get('postprocessing', 0)
                total = preprocessing + data_transfer + inference + result_fetch + postprocessing
                
                ideal_inference = path3_segmentation.get('ideal_inference', 0)
                ideal_total = preprocessing + data_transfer + ideal_inference + result_fetch + postprocessing
                
                print(f"è·¯å¾„3(tpu+tflite): é¢„å¤„ç†{preprocessing:.2f}ms + æ•°æ®ä¼ è¾“{data_transfer:.2f}ms + æ¨ç†{inference:.2f}ms(ä¸éœ€è¦åŠ è½½{ideal_inference:.2f}ms) + ç»“æœè·å–{result_fetch:.2f}ms + åå¤„ç†{postprocessing:.2f}ms = {total:.2f}ms/ä¸éœ€è¦åŠ è½½{ideal_total:.2f}ms")
            else:
                print("è·¯å¾„3(tpu+tflite): æ•°æ®ä¸å¯ç”¨")
        
        # æ­¥éª¤3ï¼šæ·±åº¦ä¼°è®¡ï¼ˆä¸‰ç§è·¯å¾„éƒ½ä½¿ç”¨ç›¸åŒçš„ä¼ ç»ŸCVæ–¹æ³•ï¼‰
        print("\næ­¥éª¤3ï¼šæ·±åº¦ä¼°è®¡")
        print("-" * 30)
        
        if "è·¯å¾„1" in results:
            # è·¯å¾„1ä½¿ç”¨processing_timesç»“æ„
            if 'processing_times' in results["è·¯å¾„1"]:
                depth_time = results["è·¯å¾„1"]['processing_times'].get('depth_estimation', 0)
                print(f"è·¯å¾„1(AART yolo+cv): {depth_time:.2f}ms")
            else:
                print("è·¯å¾„1(AART yolo+cv): æ•°æ®ä¸å¯ç”¨")
        
        if "è·¯å¾„2" in results:
            # è·¯å¾„2ä½¿ç”¨processing_timesç»“æ„ï¼ˆæ·±åº¦ä¼°è®¡ä¸ä½¿ç”¨TFLiteï¼‰
            if 'processing_times' in results["è·¯å¾„2"]:
                depth_time = results["è·¯å¾„2"]['processing_times'].get('depth_estimation', 0)
                print(f"è·¯å¾„2(cpu+tflite): {depth_time:.2f}ms")
            else:
                print("è·¯å¾„2(cpu+tflite): æ•°æ®ä¸å¯ç”¨")
        
        if "è·¯å¾„3" in results:
            # è·¯å¾„3ä½¿ç”¨processing_timesç»“æ„ï¼ˆæ·±åº¦ä¼°è®¡ä¸ä½¿ç”¨TPUï¼‰
            if 'processing_times' in results["è·¯å¾„3"]:
                depth_time = results["è·¯å¾„3"]['processing_times'].get('depth_estimation', 0)
                print(f"è·¯å¾„3(tpu+tflite): {depth_time:.2f}ms")
            else:
                print("è·¯å¾„3(tpu+tflite): æ•°æ®ä¸å¯ç”¨")
        
        # æœ€åæ€»æ—¶é—´
        print("\næœ€åæ€»æ—¶é—´")
        print("-" * 30)
        
        if "è·¯å¾„1" in results:
            # è·¯å¾„1ä½¿ç”¨processing_timesç»“æ„
            if 'processing_times' in results["è·¯å¾„1"]:
                detection_time = results["è·¯å¾„1"]['processing_times'].get('object_detection', 0)
                segmentation_time = results["è·¯å¾„1"]['processing_times'].get('semantic_segmentation', 0)
                depth_time = results["è·¯å¾„1"]['processing_times'].get('depth_estimation', 0)
                total_time = detection_time + segmentation_time + depth_time
                print(f"è·¯å¾„1(AART yolo+cv): æ£€æµ‹{detection_time:.2f}ms + è¯­ä¹‰åˆ†å‰²{segmentation_time:.2f}ms + æ·±åº¦ä¼°è®¡{depth_time:.2f}ms = {total_time:.2f}ms")
            else:
                print("è·¯å¾„1(AART yolo+cv): æ•°æ®ä¸å¯ç”¨")
        
        if "è·¯å¾„2" in results:
            path2_detection = results["è·¯å¾„2"].get('detailed_times', {}).get('object_detection', {})
            path2_segmentation = results["è·¯å¾„2"].get('detailed_times', {}).get('semantic_segmentation', {})
            
            detection_time = sum(path2_detection.values()) if path2_detection else 0
            segmentation_time = sum(path2_segmentation.values()) if path2_segmentation else 0
            depth_time = results["è·¯å¾„2"].get('processing_times', {}).get('depth_estimation', 0)
            total_time = detection_time + segmentation_time + depth_time
            
            print(f"è·¯å¾„2(cpu+tflite): æ£€æµ‹{detection_time:.2f}ms + è¯­ä¹‰åˆ†å‰²{segmentation_time:.2f}ms + æ·±åº¦ä¼°è®¡{depth_time:.2f}ms = {total_time:.2f}ms")
        
        if "è·¯å¾„3" in results:
            path3_detection = results["è·¯å¾„3"].get('detailed_times', {}).get('object_detection', {})
            path3_segmentation = results["è·¯å¾„3"].get('detailed_times', {}).get('semantic_segmentation', {})
            
            detection_time = sum(v for k, v in path3_detection.items() if not k.startswith('ideal')) if path3_detection else 0
            segmentation_time = sum(v for k, v in path3_segmentation.items() if not k.startswith('ideal')) if path3_segmentation else 0
            depth_time = results["è·¯å¾„3"].get('processing_times', {}).get('depth_estimation', 0)
            total_time = detection_time + segmentation_time + depth_time
            
            # ç†æƒ³çŠ¶æ€æ—¶é—´
            ideal_detection = (path3_detection.get('preprocessing', 0) + path3_detection.get('data_transfer', 0) + 
                              path3_detection.get('ideal_inference', 0) + path3_detection.get('result_fetch', 0) + 
                              path3_detection.get('postprocessing', 0)) if path3_detection else 0
            ideal_segmentation = (path3_segmentation.get('preprocessing', 0) + path3_segmentation.get('data_transfer', 0) + 
                                 path3_segmentation.get('ideal_inference', 0) + path3_segmentation.get('result_fetch', 0) + 
                                 path3_segmentation.get('postprocessing', 0)) if path3_segmentation else 0
            ideal_depth = depth_time  # æ·±åº¦ä¼°è®¡ä½¿ç”¨ç›¸åŒçš„ä¼ ç»ŸCVæ–¹æ³•ï¼Œæ²¡æœ‰ç†æƒ³çŠ¶æ€åŒºåˆ«
            ideal_total = ideal_detection + ideal_segmentation + ideal_depth
            
            print(f"è·¯å¾„3(tpu+tflite): æ£€æµ‹{detection_time:.2f}ms + è¯­ä¹‰åˆ†å‰²{segmentation_time:.2f}ms + æ·±åº¦ä¼°è®¡{depth_time:.2f}ms = {total_time:.2f}ms, ç†æƒ³çŠ¶æ€: æ£€æµ‹{ideal_detection:.2f}ms + è¯­ä¹‰åˆ†å‰²{ideal_segmentation:.2f}ms + æ·±åº¦ä¼°è®¡{ideal_depth:.2f}ms = {ideal_total:.2f}ms")
    
    # ä¿å­˜ç»“æœ
    if results:
        timestamp = int(time.time())
        filename = f"/ros2_ws/three_paths_comparison_{timestamp}.json"
        
        # å‡†å¤‡å¯åºåˆ—åŒ–çš„æ•°æ®
        serializable_results = {}
        for path_name, data in results.items():
            serializable_results[path_name] = {
                'description': data['description'],
                'times': [float(t) for t in data['times']],
                'avg_time': float(data['avg_time']),
                'std_time': float(data['std_time']),
                'total_detections': data['total_detections'],
                'num_tests': data['num_tests']
            }
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(serializable_results, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {filename}")
    
    print("\nâœ… æ‰€æœ‰æµ‹è¯•å®Œæˆï¼")

if __name__ == "__main__":
    main() 